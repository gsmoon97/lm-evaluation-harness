"""
Usage:
   python parse_results_wic.py --data_path <path_to_directory_of_jsonl_file>
"""
import argparse
import json
import math
import os
import random
import warnings

random.seed(42) # fix the seed for reproducibility

def find_answer_idx(responses, choices=None, tokenizer=None):
    candidates = []
    max_val = -math.inf
    # normalize logits based on prediction token length
    if choices and tokenizer:
        tokens = tokenizer(choices)['input_ids']
    for idx, r in enumerate(responses):
        # val = math.exp(r[0])
        val = r[0]
        # normalize logits based on prediction token length
        if choices and tokenizer:
            val = val/float(len(tokens[idx]))
        if val == max_val:
            candidates.append((idx,r[1]))
        elif val > max_val:
            max_val = val
            candidates = [(idx,r[1])]
    
    # check if True value exists in 'responses' object (i.e., if any of the multiple-choice options would be generated by greedy sampling from the LM)
    if any([r[1] for r in responses]):
        # check if the multiple-choice option with the True value is included in 'candidates' (i.e., if the multiple-choice option that would be generated by greedy sampling from the LM does not have the highest logit)
        if not any([c[1] for c in candidates]):
            warnings.warn(f"!!!Choice with the highest loglikelihood is not selected as the answer for this question: {responses}!!!\n")
            # randomly select one multiple-choice option from all the multiple-choice options with the highest logits as the final answer
            answer_idx = random.choice([c[0] for c in candidates])
        else:
            # choose the multiple-choice option with True value as the answer (i.e., choose the multiple-choice option that would be generated by greedy sampling from the LM as the answer)
            for c in candidates:
                if c[1]:
                    answer_idx = c[0]
                    break
    else:
        # randomly select one multiple-choice option from all the multiple-choice options with the highest logits as the final answer
        answer_idx = random.choice([c[0] for c in candidates])
        
    assert candidates != [], f"No answer is predicted for this question: {candidates}"
    return answer_idx


def main(args):
    tp = 0
    fp = 0
    valid_responses = ['yes', 'no']
    jsonl_files = [name for name in os.listdir(args.data_path) if name.endswith('.jsonl')]
    assert len(jsonl_files) <= 1, f'Found more than one .jsonl file in {args.data_path}!'
    with open(os.path.join(args.data_path, jsonl_files[0]), "r") as f:
        data = json.load(f)

    num_invalid_answers = 0
    id2answer_sense_key = {}
    for d in data:
        id = d['doc']['id']
        if args.model_path:
            answer_idx = find_answer_idx(d['filtered_resps'], choices=d['doc']['choices'], tokenizer=tokenizer)
        else:
            answer_idx = find_answer_idx(d['filtered_resps'])
        key_dict_json = json.loads(d['doc']['key_dict'])
        answer_choice = d['doc']['choices'][answer_idx]
        answer_sense_key = ""
        for sense_key, choice in key_dict_json.items():
            if choice == answer_choice:
                answer_sense_key = sense_key
                break
        if not answer_sense_key:
            warnings.warn(f'!!!No valid answer for "{id}"!!!\n')
            num_invalid_answers += 1
        id2answer_sense_key[id] = answer_sense_key
    print(f'Total number of invalid answers : {num_invalid_answers:,}')

    for subdir in os.listdir(args.data_path):
    
        try:
            full_path = os.path.join(args.data_path, subdir)
            with open(full_path, encoding='utf-8') as f:
                try:
                    datum = json.load(f)
                    answer = datum['answer']
                    raw_response = datum['raw_response']
                    response = raw_response.lower().rstrip('.')
                    if answer == response:
                        tp += 1
                    else:
                        fp += 1
                        if response not in valid_responses: # in case of invalid response
                            warnings.warn(f'[ERROR] invalid response "{response}" Please ensure that the response is one of the followings: {", ".join(valid_responses)}')
                            print(f'answer : {answer}')
                            print(f'response : {response}')
                            print('=====')
                except:
                    warnings.warn(f'[ERROR] failed to read {subdir}')
        except Exception as e:
            warnings.warn(f'[ERROR] at {subdir}:\n{e}')
    
    print(f'Precision: {(tp)/(tp+fp)*100.0:.2f}')
    
    output_path = args.output_path or os.path.join(os.path.realpath(os.path.dirname(__file__)), 'score.txt')
    with open(output_path, 'w', encoding='utf-8') as out:
        out.write(f'Precision: {(tp)/(tp+fp)*100.0:.2f}')


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help='directory to the lm-eval-harness output files.')
    parser.add_argument("--output_path", type=str, default=None, help='file path for the parsed file.')
    args = parser.parse_args()
    main(args)